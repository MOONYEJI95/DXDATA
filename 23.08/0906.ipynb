{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파이썬≥3.5 필수\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "# 공통 모듈 임포트\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "# 깔끔한 그래프 출력을 위해 %matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# 그림을 저장할 위치\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"classification\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"그림 저장:\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)\n",
    "from matplotlib import font_manager, rc\n",
    "import platform\n",
    "path = \"c:/Windows/Fonts/malgun.ttf\"\n",
    "if platform.system() == 'Darwin':\n",
    "    rc('font', family='AppleGothic')\n",
    "elif platform.system() == 'Windows':\n",
    "    font_name = font_manager.FontProperties(fname=path).get_name()\n",
    "    rc('font', family=font_name)\n",
    "\n",
    "mpl.rcParams['axes.unicode_minus'] = False\n",
    "# Jupyter Notebook의 출력을 소수점 이하 3자리로 제한\n",
    "%precision 3\n",
    "# 그래픽 출력을 조금 더 고급화하기 위한 라이브러리\n",
    "import seaborn as sns\n",
    "\n",
    "#과학 기술 통계 라이브러리\n",
    "import scipy as sp\n",
    "from scipy import stats\n",
    "# 사이킷런 ≥0.20 필수 : 0.20에서 데이터 변환을 위한 Transformer클래스가 추가됨\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "# 데이터를 분할할 때 동일한 분할을 만들기 위해서\n",
    "# 모델을 만드는 작업을 여러 번에 걸쳐서 하는 경우 시드가 변경이 되서 훈련용 데이터가\n",
    "# 자주 변경이 되면 결국 모든 데이터를 가지고 모델을 생성하는 결과\n",
    "# Overfit이 될 가능성이 높아짐\n",
    "np.random.seed(42)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 뉴스 그룹 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR'])\n"
     ]
    }
   ],
   "source": [
    "# 데이터 가져오기\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "news_data = fetch_20newsgroups(subset='all', random_state=42)\n",
    "# sklearn에서는 datasets 서브 패키지를 이용해서 가져온 데이터는 dict\n",
    "# 가져온 데이터의 key를 확인\n",
    "print(news_data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C:\\\\Users\\\\USER\\\\scikit_learn_data\\\\20news_home\\\\20news-bydate-test\\\\rec.sport.hockey\\\\54367'\n",
      " 'C:\\\\Users\\\\USER\\\\scikit_learn_data\\\\20news_home\\\\20news-bydate-train\\\\comp.sys.ibm.pc.hardware\\\\60215'\n",
      " 'C:\\\\Users\\\\USER\\\\scikit_learn_data\\\\20news_home\\\\20news-bydate-train\\\\talk.politics.mideast\\\\76120'\n",
      " ...\n",
      " 'C:\\\\Users\\\\USER\\\\scikit_learn_data\\\\20news_home\\\\20news-bydate-train\\\\comp.sys.ibm.pc.hardware\\\\60695'\n",
      " 'C:\\\\Users\\\\USER\\\\scikit_learn_data\\\\20news_home\\\\20news-bydate-train\\\\comp.graphics\\\\38319'\n",
      " 'C:\\\\Users\\\\USER\\\\scikit_learn_data\\\\20news_home\\\\20news-bydate-test\\\\rec.autos\\\\103195']\n",
      ".. _20newsgroups_dataset:\n",
      "\n",
      "The 20 newsgroups text dataset\n",
      "------------------------------\n",
      "\n",
      "The 20 newsgroups dataset comprises around 18000 newsgroups posts on\n",
      "20 topics split in two subsets: one for training (or development)\n",
      "and the other one for testing (or for performance evaluation). The split\n",
      "between the train and test set is based upon a messages posted before\n",
      "and after a specific date.\n",
      "\n",
      "This module contains two loaders. The first one,\n",
      ":func:`sklearn.datasets.fetch_20newsgroups`,\n",
      "returns a list of the raw texts that can be fed to text feature\n",
      "extractors such as :class:`~sklearn.feature_extraction.text.CountVectorizer`\n",
      "with custom parameters so as to extract feature vectors.\n",
      "The second one, :func:`sklearn.datasets.fetch_20newsgroups_vectorized`,\n",
      "returns ready-to-use features, i.e., it is not necessary to use a feature\n",
      "extractor.\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    =================   ==========\n",
      "    Classes                     20\n",
      "    Samples total            18846\n",
      "    Dimensionality               1\n",
      "    Features                  text\n",
      "    =================   ==========\n",
      "\n",
      "Usage\n",
      "~~~~~\n",
      "\n",
      "The :func:`sklearn.datasets.fetch_20newsgroups` function is a data\n",
      "fetching / caching functions that downloads the data archive from\n",
      "the original `20 newsgroups website`_, extracts the archive contents\n",
      "in the ``~/scikit_learn_data/20news_home`` folder and calls the\n",
      ":func:`sklearn.datasets.load_files` on either the training or\n",
      "testing set folder, or both of them::\n",
      "\n",
      "  >>> from sklearn.datasets import fetch_20newsgroups\n",
      "  >>> newsgroups_train = fetch_20newsgroups(subset='train')\n",
      "\n",
      "  >>> from pprint import pprint\n",
      "  >>> pprint(list(newsgroups_train.target_names))\n",
      "  ['alt.atheism',\n",
      "   'comp.graphics',\n",
      "   'comp.os.ms-windows.misc',\n",
      "   'comp.sys.ibm.pc.hardware',\n",
      "   'comp.sys.mac.hardware',\n",
      "   'comp.windows.x',\n",
      "   'misc.forsale',\n",
      "   'rec.autos',\n",
      "   'rec.motorcycles',\n",
      "   'rec.sport.baseball',\n",
      "   'rec.sport.hockey',\n",
      "   'sci.crypt',\n",
      "   'sci.electronics',\n",
      "   'sci.med',\n",
      "   'sci.space',\n",
      "   'soc.religion.christian',\n",
      "   'talk.politics.guns',\n",
      "   'talk.politics.mideast',\n",
      "   'talk.politics.misc',\n",
      "   'talk.religion.misc']\n",
      "\n",
      "The real data lies in the ``filenames`` and ``target`` attributes. The target\n",
      "attribute is the integer index of the category::\n",
      "\n",
      "  >>> newsgroups_train.filenames.shape\n",
      "  (11314,)\n",
      "  >>> newsgroups_train.target.shape\n",
      "  (11314,)\n",
      "  >>> newsgroups_train.target[:10]\n",
      "  array([ 7,  4,  4,  1, 14, 16, 13,  3,  2,  4])\n",
      "\n",
      "It is possible to load only a sub-selection of the categories by passing the\n",
      "list of the categories to load to the\n",
      ":func:`sklearn.datasets.fetch_20newsgroups` function::\n",
      "\n",
      "  >>> cats = ['alt.atheism', 'sci.space']\n",
      "  >>> newsgroups_train = fetch_20newsgroups(subset='train', categories=cats)\n",
      "\n",
      "  >>> list(newsgroups_train.target_names)\n",
      "  ['alt.atheism', 'sci.space']\n",
      "  >>> newsgroups_train.filenames.shape\n",
      "  (1073,)\n",
      "  >>> newsgroups_train.target.shape\n",
      "  (1073,)\n",
      "  >>> newsgroups_train.target[:10]\n",
      "  array([0, 1, 1, 1, 0, 1, 1, 0, 0, 0])\n",
      "\n",
      "Converting text to vectors\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "In order to feed predictive or clustering models with the text data,\n",
      "one first need to turn the text into vectors of numerical values suitable\n",
      "for statistical analysis. This can be achieved with the utilities of the\n",
      "``sklearn.feature_extraction.text`` as demonstrated in the following\n",
      "example that extract `TF-IDF`_ vectors of unigram tokens\n",
      "from a subset of 20news::\n",
      "\n",
      "  >>> from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "  >>> categories = ['alt.atheism', 'talk.religion.misc',\n",
      "  ...               'comp.graphics', 'sci.space']\n",
      "  >>> newsgroups_train = fetch_20newsgroups(subset='train',\n",
      "  ...                                       categories=categories)\n",
      "  >>> vectorizer = TfidfVectorizer()\n",
      "  >>> vectors = vectorizer.fit_transform(newsgroups_train.data)\n",
      "  >>> vectors.shape\n",
      "  (2034, 34118)\n",
      "\n",
      "The extracted TF-IDF vectors are very sparse, with an average of 159 non-zero\n",
      "components by sample in a more than 30000-dimensional space\n",
      "(less than .5% non-zero features)::\n",
      "\n",
      "  >>> vectors.nnz / float(vectors.shape[0])\n",
      "  159.01327...\n",
      "\n",
      ":func:`sklearn.datasets.fetch_20newsgroups_vectorized` is a function which\n",
      "returns ready-to-use token counts features instead of file names.\n",
      "\n",
      ".. _`20 newsgroups website`: http://people.csail.mit.edu/jrennie/20Newsgroups/\n",
      ".. _`TF-IDF`: https://en.wikipedia.org/wiki/Tf-idf\n",
      "\n",
      "\n",
      "Filtering text for more realistic training\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "It is easy for a classifier to overfit on particular things that appear in the\n",
      "20 Newsgroups data, such as newsgroup headers. Many classifiers achieve very\n",
      "high F-scores, but their results would not generalize to other documents that\n",
      "aren't from this window of time.\n",
      "\n",
      "For example, let's look at the results of a multinomial Naive Bayes classifier,\n",
      "which is fast to train and achieves a decent F-score::\n",
      "\n",
      "  >>> from sklearn.naive_bayes import MultinomialNB\n",
      "  >>> from sklearn import metrics\n",
      "  >>> newsgroups_test = fetch_20newsgroups(subset='test',\n",
      "  ...                                      categories=categories)\n",
      "  >>> vectors_test = vectorizer.transform(newsgroups_test.data)\n",
      "  >>> clf = MultinomialNB(alpha=.01)\n",
      "  >>> clf.fit(vectors, newsgroups_train.target)\n",
      "  MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)\n",
      "\n",
      "  >>> pred = clf.predict(vectors_test)\n",
      "  >>> metrics.f1_score(newsgroups_test.target, pred, average='macro')\n",
      "  0.88213...\n",
      "\n",
      "(The example :ref:`sphx_glr_auto_examples_text_plot_document_classification_20newsgroups.py` shuffles\n",
      "the training and test data, instead of segmenting by time, and in that case\n",
      "multinomial Naive Bayes gets a much higher F-score of 0.88. Are you suspicious\n",
      "yet of what's going on inside this classifier?)\n",
      "\n",
      "Let's take a look at what the most informative features are:\n",
      "\n",
      "  >>> import numpy as np\n",
      "  >>> def show_top10(classifier, vectorizer, categories):\n",
      "  ...     feature_names = vectorizer.get_feature_names_out()\n",
      "  ...     for i, category in enumerate(categories):\n",
      "  ...         top10 = np.argsort(classifier.coef_[i])[-10:]\n",
      "  ...         print(\"%s: %s\" % (category, \" \".join(feature_names[top10])))\n",
      "  ...\n",
      "  >>> show_top10(clf, vectorizer, newsgroups_train.target_names)\n",
      "  alt.atheism: edu it and in you that is of to the\n",
      "  comp.graphics: edu in graphics it is for and of to the\n",
      "  sci.space: edu it that is in and space to of the\n",
      "  talk.religion.misc: not it you in is that and to of the\n",
      "\n",
      "\n",
      "You can now see many things that these features have overfit to:\n",
      "\n",
      "- Almost every group is distinguished by whether headers such as\n",
      "  ``NNTP-Posting-Host:`` and ``Distribution:`` appear more or less often.\n",
      "- Another significant feature involves whether the sender is affiliated with\n",
      "  a university, as indicated either by their headers or their signature.\n",
      "- The word \"article\" is a significant feature, based on how often people quote\n",
      "  previous posts like this: \"In article [article ID], [name] <[e-mail address]>\n",
      "  wrote:\"\n",
      "- Other features match the names and e-mail addresses of particular people who\n",
      "  were posting at the time.\n",
      "\n",
      "With such an abundance of clues that distinguish newsgroups, the classifiers\n",
      "barely have to identify topics from text at all, and they all perform at the\n",
      "same high level.\n",
      "\n",
      "For this reason, the functions that load 20 Newsgroups data provide a\n",
      "parameter called **remove**, telling it what kinds of information to strip out\n",
      "of each file. **remove** should be a tuple containing any subset of\n",
      "``('headers', 'footers', 'quotes')``, telling it to remove headers, signature\n",
      "blocks, and quotation blocks respectively.\n",
      "\n",
      "  >>> newsgroups_test = fetch_20newsgroups(subset='test',\n",
      "  ...                                      remove=('headers', 'footers', 'quotes'),\n",
      "  ...                                      categories=categories)\n",
      "  >>> vectors_test = vectorizer.transform(newsgroups_test.data)\n",
      "  >>> pred = clf.predict(vectors_test)\n",
      "  >>> metrics.f1_score(pred, newsgroups_test.target, average='macro')\n",
      "  0.77310...\n",
      "\n",
      "This classifier lost over a lot of its F-score, just because we removed\n",
      "metadata that has little to do with topic classification.\n",
      "It loses even more if we also strip this metadata from the training data:\n",
      "\n",
      "  >>> newsgroups_train = fetch_20newsgroups(subset='train',\n",
      "  ...                                       remove=('headers', 'footers', 'quotes'),\n",
      "  ...                                       categories=categories)\n",
      "  >>> vectors = vectorizer.fit_transform(newsgroups_train.data)\n",
      "  >>> clf = MultinomialNB(alpha=.01)\n",
      "  >>> clf.fit(vectors, newsgroups_train.target)\n",
      "  MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)\n",
      "\n",
      "  >>> vectors_test = vectorizer.transform(newsgroups_test.data)\n",
      "  >>> pred = clf.predict(vectors_test)\n",
      "  >>> metrics.f1_score(newsgroups_test.target, pred, average='macro')\n",
      "  0.76995...\n",
      "\n",
      "Some other classifiers cope better with this harder version of the task. Try the\n",
      ":ref:`sphx_glr_auto_examples_model_selection_plot_grid_search_text_feature_extraction.py`\n",
      "example with and without the `remove` option to compare the results.\n",
      "\n",
      ".. topic:: Data Considerations\n",
      "\n",
      "  The Cleveland Indians is a major league baseball team based in Cleveland,\n",
      "  Ohio, USA. In December 2020, it was reported that \"After several months of\n",
      "  discussion sparked by the death of George Floyd and a national reckoning over\n",
      "  race and colonialism, the Cleveland Indians have decided to change their\n",
      "  name.\" Team owner Paul Dolan \"did make it clear that the team will not make\n",
      "  its informal nickname -- the Tribe -- its new team name.\" \"It's not going to\n",
      "  be a half-step away from the Indians,\" Dolan said.\"We will not have a Native\n",
      "  American-themed name.\"\n",
      "\n",
      "  https://www.mlb.com/news/cleveland-indians-team-name-change\n",
      "\n",
      ".. topic:: Recommendation\n",
      "\n",
      "  - When evaluating text classifiers on the 20 Newsgroups data, you\n",
      "    should strip newsgroup-related metadata. In scikit-learn, you can do this\n",
      "    by setting ``remove=('headers', 'footers', 'quotes')``. The F-score will be\n",
      "    lower because it is more realistic.\n",
      "  - This text dataset contains data which may be inappropriate for certain NLP\n",
      "    applications. An example is listed in the \"Data Considerations\" section\n",
      "    above. The challenge with using current text datasets in NLP for tasks such\n",
      "    as sentence completion, clustering, and other applications is that text\n",
      "    that is culturally biased and inflammatory will propagate biases. This\n",
      "    should be taken into consideration when using the dataset, reviewing the\n",
      "    output, and the bias should be documented.\n",
      "\n",
      ".. topic:: Examples\n",
      "\n",
      "   * :ref:`sphx_glr_auto_examples_model_selection_plot_grid_search_text_feature_extraction.py`\n",
      "\n",
      "   * :ref:`sphx_glr_auto_examples_text_plot_document_classification_20newsgroups.py`\n",
      "\n",
      "   * :ref:`sphx_glr_auto_examples_text_plot_hashing_vs_dict_vectorizer.py`\n",
      "\n",
      "   * :ref:`sphx_glr_auto_examples_text_plot_document_clustering.py`\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(news_data.filenames)\n",
    "print(news_data.DESCR) #데이터에 대한 설명"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     799\n",
      "1     973\n",
      "2     985\n",
      "3     982\n",
      "4     963\n",
      "5     988\n",
      "6     975\n",
      "7     990\n",
      "8     996\n",
      "9     994\n",
      "10    999\n",
      "11    991\n",
      "12    984\n",
      "13    990\n",
      "14    987\n",
      "15    997\n",
      "16    910\n",
      "17    940\n",
      "18    775\n",
      "19    628\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 분포 확인 - 분포가 한 쪽으로 치우치게 되면 데이터를 층화 추출을 할 것인지 아니면 오버 샘플링이나 언더 샘플링인지 또는 로그 변환을 할 것인지 선택\n",
    "print(pd.Series(news_data.target).value_counts().sort_index())\n",
    "# 타겟은 숫자로 되어 있음을 확인함\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n",
      "From: Mamatha Devineni Ratnam <mr47+@andrew.cmu.edu>\n",
      "Subject: Pens fans reactions\n",
      "Organization: Post Office, Carnegie Mellon, Pittsburgh, PA\n",
      "Lines: 12\n",
      "NNTP-Posting-Host: po4.andrew.cmu.edu\n",
      "\n",
      "\n",
      "\n",
      "I am sure some bashers of Pens fans are pretty confused about the lack\n",
      "of any kind of posts about the recent Pens massacre of the Devils. Actually,\n",
      "I am  bit puzzled too and a bit relieved. However, I am going to put an end\n",
      "to non-PIttsburghers' relief with a bit of praise for the Pens. Man, they\n",
      "are killing those Devils worse than I thought. Jagr just showed you why\n",
      "he is much better than his regular season stats. He is also a lot\n",
      "fo fun to watch in the playoffs. Bowman should let JAgr have a lot of\n",
      "fun in the next couple of games since the Pens are going to beat the pulp out of Jersey anyway. I was very disappointed not to see the Islanders lose the final\n",
      "regular season game.          PENS RULE!!!\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# target의 클래스 이름확인\n",
    "print(news_data.target_names)\n",
    "# 데이터 확인\n",
    "print(news_data.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "# 데이터 가져오기\n",
    "# headers 나 footers, quotes를 제거하고 가져오기\n",
    "train_news = fetch_20newsgroups(subset='train', remove=('headers','footers','quotes'),\n",
    "                                random_state=42)\n",
    "\n",
    "# 훈련 데이터 생성\n",
    "X_train = train_news.data\n",
    "y_train = train_news.target # 실제 데이터에서 이게 없으면 군집\n",
    "print(type(X_train))\n",
    "\n",
    "test_news = fetch_20newsgroups(subset='test', remove=('headers','footers','quotes'),\n",
    "                                random_state=42)\n",
    "\n",
    "X_test = test_news.data\n",
    "y_test = test_news.target # 실제 데이터에서 이게 없으면 군집\n",
    "print(type(X_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 피처 벡터화 - 문자열을 벡터화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 데이터 Text의 CountVectorizer Shape: (11314, 101631)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Count Vectorization으로 feature extraction 변환 수행.\n",
    "# 단어가 등장한 개수 기반의 벡터화를 위한 인스턴스 생성\n",
    "cnt_vect = CountVectorizer()\n",
    "\n",
    "# 벡터화\n",
    "cnt_vect.fit(X_train)\n",
    "X_train_cnt_vect = cnt_vect.transform(X_train)\n",
    "# print(type(X_train_cnt_vect)) # 타입은 희소 행렬\n",
    "\n",
    "# 학습 데이터로 fit( )된 CountVectorizer를 이용하여 테스트 데이터를 feature extraction 변환 수행. \n",
    "X_test_cnt_vect = cnt_vect.transform(X_test)\n",
    "\n",
    "print('학습 데이터 Text의 CountVectorizer Shape:',X_train_cnt_vect.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 로지스틱 회귀를 이용한 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorized Logistic Regression 의 예측 정확도는 0.597\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# LogisticRegression을 이용하여 학습/예측/평가 수행. \n",
    "lr_clf = LogisticRegression(solver=\"lbfgs\", max_iter=1000)\n",
    "lr_clf.fit(X_train_cnt_vect , y_train)\n",
    "pred = lr_clf.predict(X_test_cnt_vect)\n",
    "print('CountVectorized Logistic Regression 의 예측 정확도는 {0:.3f}'.format(accuracy_score(y_test,pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Vectorized Logistic Regression 의 예측 정확도는 0.674\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vect = TfidfVectorizer()\n",
    "tfidf_vect.fit(X_train)\n",
    "X_train_tfidf_vect = tfidf_vect.transform(X_train)\n",
    "X_test_tfidf_vect = tfidf_vect.transform(X_test)\n",
    "\n",
    "lr_clf = LogisticRegression(solver=\"lbfgs\", max_iter=1000)\n",
    "lr_clf.fit(X_train_tfidf_vect , y_train)\n",
    "pred = lr_clf.predict(X_test_tfidf_vect)\n",
    "print('TF-IDF Logistic Regression 의 예측 정확도는 {0:.3f}'.format(accuracy_score(y_test ,pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Vectorized Logistic Regression 의 예측 정확도는 0.692\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF에 파라미터를 설정하기(stop_words='english', ngram_range=(1,2), max_df=300)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vect = TfidfVectorizer(stop_words='english', ngram_range=(1,2), max_df=300 )\n",
    "tfidf_vect.fit(X_train)\n",
    "X_train_tfidf_vect = tfidf_vect.transform(X_train)\n",
    "X_test_tfidf_vect = tfidf_vect.transform(X_test)\n",
    "\n",
    "lr_clf = LogisticRegression(solver=\"lbfgs\", max_iter=1000)\n",
    "lr_clf.fit(X_train_tfidf_vect , y_train)\n",
    "pred = lr_clf.predict(X_test_tfidf_vect)\n",
    "print('TF-IDF Vectorized Logistic Regression 의 예측 정확도는 {0:.3f}'.format(accuracy_score(y_test ,pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 감성 분석"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 나이브 베이즈를 이용한 감성 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'like', 'not', 'do', 'i', 'hate', 'you', 'love'}\n"
     ]
    }
   ],
   "source": [
    "### 훈련 데이터 만들기\n",
    "train = [('i like you', 'pos'), \n",
    "         ('i do not like you', 'neg'),\n",
    "         ('i hate you', 'neg'), \n",
    "         ('i do not hate you', 'pos'),\n",
    "        ('i love you', 'pos'),\n",
    "        ('I do not love you', 'neg')]\n",
    "\n",
    "# 등장한 모든 단어 찾기\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "# 단어 단위로 분할해서 등장한 단어 전부 추출\n",
    "all_words = set(word.lower() for sentence in train for word in word_tokenize(sentence[0]))\n",
    "print(all_words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[({'like': True, 'not': False, 'do': False, 'i': True, 'hate': False, 'you': True, 'love': False}, 'pos'), ({'like': True, 'not': True, 'do': True, 'i': True, 'hate': False, 'you': True, 'love': False}, 'neg'), ({'like': False, 'not': False, 'do': False, 'i': True, 'hate': True, 'you': True, 'love': False}, 'neg'), ({'like': False, 'not': True, 'do': True, 'i': True, 'hate': True, 'you': True, 'love': False}, 'pos'), ({'like': False, 'not': False, 'do': False, 'i': True, 'hate': False, 'you': True, 'love': True}, 'pos'), ({'like': False, 'not': True, 'do': True, 'i': False, 'hate': False, 'you': True, 'love': True}, 'neg')]\n"
     ]
    }
   ],
   "source": [
    "# 분류기 만들기\n",
    "\n",
    "# 단어 토큰화 - 각 문장에 단어의 포함 여부를 만들고 감성을 기록\n",
    "# 위에서 등장한 단어를 추출한 것으로(총7개 단어) True, False로 포함여부를 판단\n",
    "t = [({word: (word in word_tokenize(x[0])) for word in all_words}, x[1])\n",
    "                                                        for x in train]\n",
    "print(t)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                      do = False             pos : neg    =      1.7 : 1.0\n",
      "                      do = True              neg : pos    =      1.7 : 1.0\n",
      "                     not = False             pos : neg    =      1.7 : 1.0\n",
      "                     not = True              neg : pos    =      1.7 : 1.0\n",
      "                       i = True              pos : neg    =      1.4 : 1.0\n",
      "                    hate = False             neg : pos    =      1.0 : 1.0\n",
      "                    hate = True              neg : pos    =      1.0 : 1.0\n",
      "                    like = False             neg : pos    =      1.0 : 1.0\n",
      "                    like = True              neg : pos    =      1.0 : 1.0\n"
     ]
    }
   ],
   "source": [
    "# 텍스트 분류를 위한 나이브베이즈 분류기를 이용해서 모델을 생성\n",
    "classifier = nltk.NaiveBayesClassifier.train(t)\n",
    "classifier.show_most_informative_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'like': True, 'not': True, 'do': True, 'i': True, 'hate': False, 'you': False, 'love': False}\n",
      "neg\n"
     ]
    }
   ],
   "source": [
    "# 예측\n",
    "# 샘플 문장 테스트 \n",
    "test_sentence = 'i do not like jessica'\n",
    "test_sent_features = {word.lower():(word in word_tokenize(test_sentence.lower()))\n",
    "                     for word in all_words}\n",
    "print(test_sent_features)\n",
    "print(classifier.classify(test_sent_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 한글 감성 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'함께', '즐겁습니다', '사랑하지', '당신이', '힘들어', '않습니다', '너무', '당신을', '나는', '좋지', '노는', '지루하지', '않아요', '하는', '제시카와', '즐겁지', '있는', '일을', '사랑합니다', '당신과', '일이', '좋습니다', '만나는', '것이', '지루합니다'}\n"
     ]
    }
   ],
   "source": [
    "train = [('나는 당신을 사랑합니다', 'pos'), \n",
    "         ('나는 당신을 사랑하지 않아요', 'neg'),\n",
    "         ('나는 당신을 만나는 것이 지루합니다', 'neg'),\n",
    "         ('나는 당신을 만나는 것이 지루하지 않습니다', 'pos'),\n",
    "         ('나는 당신이 좋습니다', 'pos'),\n",
    "         ('나는 당신이 좋지 않습니다', 'neg'),\n",
    "         ('나는 당신과 노는 것이 즐겁습니다', 'pos'),\n",
    "         ('나는 당신과 노는 것이 즐겁지 않습니다', 'neg'),\n",
    "        ('나는 제시카와 함께 있는 것이 즐겁습니다', 'pos'),\n",
    "        ('나는 일을 하는 것이 즐겁지 않습니다', 'neg'),\n",
    "         ('나는 일이 너무 힘들어', 'neg')]\n",
    "\n",
    "#단순 단어 분류 – 조사가 다른 경우 다른 단어로 구분됨'\n",
    "#영어가 있을 수도 있어서 lower 써주기\n",
    "#단어 단위로 분할해서 등장한 단어 전부 추출\n",
    "all_words = set(word.lower() for sentence in train\n",
    "                        for word in word_tokenize(sentence[0]))\n",
    "print(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['나/Noun', '는/Josa', '당신/Noun', '을/Josa', '사랑/Noun', '하다/Verb'], 'pos'), (['나/Noun', '는/Josa', '당신/Noun', '을/Josa', '사랑/Noun', '하다/Verb', '않다/Verb'], 'neg'), (['나/Noun', '는/Josa', '당신/Noun', '을/Josa', '만나다/Verb', '것/Noun', '이/Josa', '지루하다/Adjective'], 'neg'), (['나/Noun', '는/Josa', '당신/Noun', '을/Josa', '만나다/Verb', '것/Noun', '이/Josa', '지루하다/Adjective', '않다/Verb'], 'pos'), (['나/Noun', '는/Josa', '당신/Noun', '이/Josa', '좋다/Adjective'], 'pos'), (['나/Noun', '는/Josa', '당신/Noun', '이/Josa', '좋다/Adjective', '않다/Verb'], 'neg'), (['나/Noun', '는/Josa', '당신/Noun', '과/Josa', '노/Noun', '는/Josa', '것/Noun', '이/Josa', '즐겁다/Adjective'], 'pos'), (['나/Noun', '는/Josa', '당신/Noun', '과/Josa', '노/Noun', '는/Josa', '것/Noun', '이/Josa', '즐겁다/Adjective', '않다/Verb'], 'neg'), (['나/Noun', '는/Josa', '제시카/Noun', '와/Josa', '함께/Adverb', '있다/Adjective', '것/Noun', '이/Josa', '즐겁다/Adjective'], 'pos'), (['나/Noun', '는/Josa', '일/Noun', '을/Josa', '하다/Verb', '것/Noun', '이/Josa', '즐겁다/Adjective', '않다/Verb'], 'neg'), (['나/Noun', '는/Josa', '일이/Modifier', '너무/Adverb', '힘들다/Adjective'], 'neg')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\anaconda3\\lib\\site-packages\\konlpy\\tag\\_okt.py:17: UserWarning: \"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.\n",
      "  warn('\"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.')\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Twitter\n",
    "twitter = Twitter()\n",
    "\n",
    "#문장 단위로 형태소 분석기에 넣어서 단어와 품사를 /로 구분해서 추출해주는 함수\n",
    "def tokenize(doc):\n",
    "    return [\"/\".join(t) for t in twitter.pos(doc, norm=True, stem=True)]\n",
    "\n",
    "train_docs = [(tokenize(row[0]), row[1]) for row in train]\n",
    "\n",
    "print(train_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['나/Noun', '는/Josa', '당신/Noun', '을/Josa', '사랑/Noun', '하다/Verb', '나/Noun', '는/Josa', '당신/Noun', '을/Josa', '사랑/Noun', '하다/Verb', '않다/Verb', '나/Noun', '는/Josa', '당신/Noun', '을/Josa', '만나다/Verb', '것/Noun', '이/Josa', '지루하다/Adjective', '나/Noun', '는/Josa', '당신/Noun', '을/Josa', '만나다/Verb', '것/Noun', '이/Josa', '지루하다/Adjective', '않다/Verb', '나/Noun', '는/Josa', '당신/Noun', '이/Josa', '좋다/Adjective', '나/Noun', '는/Josa', '당신/Noun', '이/Josa', '좋다/Adjective', '않다/Verb', '나/Noun', '는/Josa', '당신/Noun', '과/Josa', '노/Noun', '는/Josa', '것/Noun', '이/Josa', '즐겁다/Adjective', '나/Noun', '는/Josa', '당신/Noun', '과/Josa', '노/Noun', '는/Josa', '것/Noun', '이/Josa', '즐겁다/Adjective', '않다/Verb', '나/Noun', '는/Josa', '제시카/Noun', '와/Josa', '함께/Adverb', '있다/Adjective', '것/Noun', '이/Josa', '즐겁다/Adjective', '나/Noun', '는/Josa', '일/Noun', '을/Josa', '하다/Verb', '것/Noun', '이/Josa', '즐겁다/Adjective', '않다/Verb', '나/Noun', '는/Josa', '일이/Modifier', '너무/Adverb', '힘들다/Adjective']\n"
     ]
    }
   ],
   "source": [
    "# 단어만 추출하기(pos, neg로 분류된 부분은 빼고 단어만 추출)\n",
    "# 형태소 분류를 한 후 단어 분류\n",
    "tokens = [t for d in train_docs for t in d[0]]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[({'나/Noun': True, '는/Josa': True, '당신/Noun': True, '을/Josa': True, '사랑/Noun': True, '하다/Verb': True, '않다/Verb': False, '만나다/Verb': False, '것/Noun': False, '이/Josa': False, '지루하다/Adjective': False, '좋다/Adjective': False, '과/Josa': False, '노/Noun': False, '즐겁다/Adjective': False, '제시카/Noun': False, '와/Josa': False, '함께/Adverb': False, '있다/Adjective': False, '일/Noun': False, '일이/Modifier': False, '너무/Adverb': False, '힘들다/Adjective': False}, 'pos'), ({'나/Noun': True, '는/Josa': True, '당신/Noun': True, '을/Josa': True, '사랑/Noun': True, '하다/Verb': True, '않다/Verb': True, '만나다/Verb': False, '것/Noun': False, '이/Josa': False, '지루하다/Adjective': False, '좋다/Adjective': False, '과/Josa': False, '노/Noun': False, '즐겁다/Adjective': False, '제시카/Noun': False, '와/Josa': False, '함께/Adverb': False, '있다/Adjective': False, '일/Noun': False, '일이/Modifier': False, '너무/Adverb': False, '힘들다/Adjective': False}, 'neg'), ({'나/Noun': True, '는/Josa': True, '당신/Noun': True, '을/Josa': True, '사랑/Noun': False, '하다/Verb': False, '않다/Verb': False, '만나다/Verb': True, '것/Noun': True, '이/Josa': True, '지루하다/Adjective': True, '좋다/Adjective': False, '과/Josa': False, '노/Noun': False, '즐겁다/Adjective': False, '제시카/Noun': False, '와/Josa': False, '함께/Adverb': False, '있다/Adjective': False, '일/Noun': False, '일이/Modifier': False, '너무/Adverb': False, '힘들다/Adjective': False}, 'neg'), ({'나/Noun': True, '는/Josa': True, '당신/Noun': True, '을/Josa': True, '사랑/Noun': False, '하다/Verb': False, '않다/Verb': True, '만나다/Verb': True, '것/Noun': True, '이/Josa': True, '지루하다/Adjective': True, '좋다/Adjective': False, '과/Josa': False, '노/Noun': False, '즐겁다/Adjective': False, '제시카/Noun': False, '와/Josa': False, '함께/Adverb': False, '있다/Adjective': False, '일/Noun': False, '일이/Modifier': False, '너무/Adverb': False, '힘들다/Adjective': False}, 'pos'), ({'나/Noun': True, '는/Josa': True, '당신/Noun': True, '을/Josa': False, '사랑/Noun': False, '하다/Verb': False, '않다/Verb': False, '만나다/Verb': False, '것/Noun': False, '이/Josa': True, '지루하다/Adjective': False, '좋다/Adjective': True, '과/Josa': False, '노/Noun': False, '즐겁다/Adjective': False, '제시카/Noun': False, '와/Josa': False, '함께/Adverb': False, '있다/Adjective': False, '일/Noun': False, '일이/Modifier': False, '너무/Adverb': False, '힘들다/Adjective': False}, 'pos'), ({'나/Noun': True, '는/Josa': True, '당신/Noun': True, '을/Josa': False, '사랑/Noun': False, '하다/Verb': False, '않다/Verb': True, '만나다/Verb': False, '것/Noun': False, '이/Josa': True, '지루하다/Adjective': False, '좋다/Adjective': True, '과/Josa': False, '노/Noun': False, '즐겁다/Adjective': False, '제시카/Noun': False, '와/Josa': False, '함께/Adverb': False, '있다/Adjective': False, '일/Noun': False, '일이/Modifier': False, '너무/Adverb': False, '힘들다/Adjective': False}, 'neg'), ({'나/Noun': True, '는/Josa': True, '당신/Noun': True, '을/Josa': False, '사랑/Noun': False, '하다/Verb': False, '않다/Verb': False, '만나다/Verb': False, '것/Noun': True, '이/Josa': True, '지루하다/Adjective': False, '좋다/Adjective': False, '과/Josa': True, '노/Noun': True, '즐겁다/Adjective': True, '제시카/Noun': False, '와/Josa': False, '함께/Adverb': False, '있다/Adjective': False, '일/Noun': False, '일이/Modifier': False, '너무/Adverb': False, '힘들다/Adjective': False}, 'pos'), ({'나/Noun': True, '는/Josa': True, '당신/Noun': True, '을/Josa': False, '사랑/Noun': False, '하다/Verb': False, '않다/Verb': True, '만나다/Verb': False, '것/Noun': True, '이/Josa': True, '지루하다/Adjective': False, '좋다/Adjective': False, '과/Josa': True, '노/Noun': True, '즐겁다/Adjective': True, '제시카/Noun': False, '와/Josa': False, '함께/Adverb': False, '있다/Adjective': False, '일/Noun': False, '일이/Modifier': False, '너무/Adverb': False, '힘들다/Adjective': False}, 'neg'), ({'나/Noun': True, '는/Josa': True, '당신/Noun': False, '을/Josa': False, '사랑/Noun': False, '하다/Verb': False, '않다/Verb': False, '만나다/Verb': False, '것/Noun': True, '이/Josa': True, '지루하다/Adjective': False, '좋다/Adjective': False, '과/Josa': False, '노/Noun': False, '즐겁다/Adjective': True, '제시카/Noun': True, '와/Josa': True, '함께/Adverb': True, '있다/Adjective': True, '일/Noun': False, '일이/Modifier': False, '너무/Adverb': False, '힘들다/Adjective': False}, 'pos'), ({'나/Noun': True, '는/Josa': True, '당신/Noun': False, '을/Josa': True, '사랑/Noun': False, '하다/Verb': True, '않다/Verb': True, '만나다/Verb': False, '것/Noun': True, '이/Josa': True, '지루하다/Adjective': False, '좋다/Adjective': False, '과/Josa': False, '노/Noun': False, '즐겁다/Adjective': True, '제시카/Noun': False, '와/Josa': False, '함께/Adverb': False, '있다/Adjective': False, '일/Noun': True, '일이/Modifier': False, '너무/Adverb': False, '힘들다/Adjective': False}, 'neg'), ({'나/Noun': True, '는/Josa': True, '당신/Noun': False, '을/Josa': False, '사랑/Noun': False, '하다/Verb': False, '않다/Verb': False, '만나다/Verb': False, '것/Noun': False, '이/Josa': False, '지루하다/Adjective': False, '좋다/Adjective': False, '과/Josa': False, '노/Noun': False, '즐겁다/Adjective': False, '제시카/Noun': False, '와/Josa': False, '함께/Adverb': False, '있다/Adjective': False, '일/Noun': False, '일이/Modifier': True, '너무/Adverb': True, '힘들다/Adjective': True}, 'neg')]\n"
     ]
    }
   ],
   "source": [
    "# 분류기 만들기 - 문장에 단어의 존재 여부를 확인해주는 함수\n",
    "def term_exists(doc):\n",
    "    return {word:(word in set(doc)) for word in tokens}\n",
    "# 모든 문장을 해석해서 단어의 존재 여부와 감성을 가진 튜플의 list를 생성\n",
    "train_xy = [(term_exists(d), c) for d, c in train_docs]\n",
    "print(train_xy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                 않다/Verb = True              neg : pos    =      2.6 : 1.0\n",
      "                 않다/Verb = False             pos : neg    =      2.1 : 1.0\n",
      "                 당신/Noun = False             neg : pos    =      1.4 : 1.0\n",
      "                  이/Josa = False             neg : pos    =      1.4 : 1.0\n",
      "                 하다/Verb = True              neg : pos    =      1.4 : 1.0\n",
      "                  와/Josa = False             neg : pos    =      1.2 : 1.0\n",
      "            있다/Adjective = False             neg : pos    =      1.2 : 1.0\n",
      "                제시카/Noun = False             neg : pos    =      1.2 : 1.0\n",
      "               함께/Adverb = False             neg : pos    =      1.2 : 1.0\n",
      "                  것/Noun = False             neg : pos    =      1.2 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier = nltk.NaiveBayesClassifier.train(train_xy)\n",
    "classifier.show_most_informative_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('여섯시', 'Noun'), ('부터', 'Noun'), ('일', 'Noun'), ('을', 'Josa'), ('해야해', 'Verb')]\n",
      "{('여섯시', 'Noun'): False, ('부터', 'Noun'): False, ('일', 'Noun'): False, ('을', 'Josa'): False, ('해야해', 'Verb'): False}\n",
      "neg\n"
     ]
    }
   ],
   "source": [
    "### 샘플 문장 테스트\n",
    "test_sentence = [(\"여섯시 부터 일을 해야해\")]\n",
    "test_docs = twitter.pos(test_sentence[0])\n",
    "print(test_docs)\n",
    "test_sent_features = {word: (word in tokens) for word in test_docs}\n",
    "print(test_sent_features)\n",
    "print(classifier.classify(test_sent_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMDB 영화평 데이터를 이용한 지도 학습 기반 감성 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"5814_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"With all this stuff going down at the moment ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"2381_9\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"\\\"The Classic War of the Worlds\\\" by Timothy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"7759_3\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"The film starts with a manager (Nicholas Bell...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  sentiment                                             review\n",
       "0  \"5814_8\"          1  \"With all this stuff going down at the moment ...\n",
       "1  \"2381_9\"          1  \"\\\"The Classic War of the Worlds\\\" by Timothy ...\n",
       "2  \"7759_3\"          0  \"The film starts with a manager (Nicholas Bell..."
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_df = pd.read_csv('./data/data/data/imdb/labeledTrainData.tsv', header=0, sep=\"\\t\", quoting=3)\n",
    "review_df.head(3)\n",
    "# id : review를 구분하기 위한 데이터\n",
    "# sentiment : 감성(1이면 긍정 0이면 부정)\n",
    "# review가 review 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         id  sentiment                                             review\n",
      "0  \"5814_8\"          1   With all this stuff going down at the moment ...\n",
      "1  \"2381_9\"          1     The Classic War of the Worlds   by Timothy ...\n",
      "2  \"7759_3\"          0   The film starts with a manager  Nicholas Bell...\n",
      "3  \"3630_4\"          0   It must be assumed that those who praised thi...\n",
      "4  \"9495_8\"          1   Superbly trashy and wondrously unpretentious ...\n"
     ]
    }
   ],
   "source": [
    "# 정규식을 이용해서 불필요한 데이터 제거\n",
    "# 정규식 모듈\n",
    "import re\n",
    "\n",
    "# <br> html 태그는 replace 함수로 공백으로 변환\n",
    "review_df['review'] = review_df['review'].str.replace('<br />',' ')\n",
    "\n",
    "# 파이썬의 정규 표현식 모듈인 re를 이용하여 영어 문자열이 아닌 문자는 모두 공백으로 변환 \n",
    "review_df['review'] = review_df['review'].apply( lambda x : re.sub(\"[^a-zA-Z]\", \" \", x))\n",
    "print(review_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((17500, 1), (7500, 1))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 훈련 데이터와 테스트 데이터 분할\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class_df = review_df['sentiment']\n",
    "feature_df = review_df.drop(['id','sentiment'], axis=1, inplace=False)\n",
    "\n",
    "#print(class_df)\n",
    "#print(feature_df)\n",
    "\n",
    "X_train, X_test, y_train, y_test= train_test_split(feature_df, class_df, test_size=0.3, random_state=42)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측 정확도는 0.8847, ROC-AUC는 0.9508\n"
     ]
    }
   ],
   "source": [
    "# 훈련 및 예측\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "# 피처를 벡터화 시킨 후 로지스틱 회귀 진행 - 파이프라인으로 한번에 진행\n",
    "# 스톱 워드는 English, filtering, ngram은 (1,2)로 설정해 CountVectorization수행. \n",
    "# LogisticRegression의 C는 10으로 설정. \n",
    "pipeline = Pipeline([\n",
    "    ('cnt_vect', CountVectorizer(stop_words='english', ngram_range=(1,2) )),\n",
    "    ('lr_clf', LogisticRegression(C=10))])\n",
    "\n",
    "# Pipeline 객체를 이용하여 fit(), predict()로 학습/예측 수행. predict_proba()는 roc_auc때문에 수행.  \n",
    "pipeline.fit(X_train['review'], y_train)\n",
    "pred = pipeline.predict(X_test['review'])\n",
    "pred_probs = pipeline.predict_proba(X_test['review'])[:,1]\n",
    "\n",
    "print('예측 정확도는 {0:.4f}, ROC-AUC는 {1:.4f}'.format(accuracy_score(y_test ,pred),\n",
    "                                         roc_auc_score(y_test, pred_probs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측 정확도는 0.8916, ROC-AUC는 0.9592\n"
     ]
    }
   ],
   "source": [
    "# 벡터화를 수정(CountVectorizer -> TfidfVectorizer)\n",
    "pipeline = Pipeline([\n",
    "    ('cnt_vect', TfidfVectorizer(stop_words='english', ngram_range=(1,2) )),\n",
    "    ('lr_clf', LogisticRegression(C=10))])\n",
    "\n",
    "\n",
    "pipeline.fit(X_train['review'], y_train)\n",
    "pred = pipeline.predict(X_test['review'])\n",
    "pred_probs = pipeline.predict_proba(X_test['review'])[:,1]\n",
    "\n",
    "print('예측 정확도는 {0:.4f}, ROC-AUC는 {1:.4f}'.format(accuracy_score(y_test ,pred),\n",
    "                                         roc_auc_score(y_test, pred_probs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 네이버 식당 리뷰 데이터를 이용한 한글 지도 학습 기반의 감성 분석"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 데이터 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   score                      review  y\n",
      "0      5            친절하시고 깔끔하고 좋았습니다  1\n",
      "1      5                  조용하고 고기도 굿  1\n",
      "2      4      갈비탕과 냉면, 육회비빔밥이 맛있습니다.  1\n",
      "3      4  대체적으로 만족하나\\n와인의 구성이 살짝 아쉬움  1\n",
      "4      5       고기도 맛있고 서비스는 더 최고입니다~  1\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"./data/data/data/review_data.csv\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>y</th>\n",
       "      <th>ko_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>친절하시고 깔끔하고 좋았습니다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>조용하고 고기도 굿</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>갈비탕과 냉면 육회비빔밥이 맛있습니다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>대체적으로 만족하나와인의 구성이 살짝 아쉬움</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>고기도 맛있고 서비스는 더 최고입니다</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   score  y                   ko_text\n",
       "0      5  1          친절하시고 깔끔하고 좋았습니다\n",
       "1      5  1                조용하고 고기도 굿\n",
       "2      4  1      갈비탕과 냉면 육회비빔밥이 맛있습니다\n",
       "3      4  1  대체적으로 만족하나와인의 구성이 살짝 아쉬움\n",
       "4      5  1      고기도 맛있고 서비스는 더 최고입니다"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# 텍스트 정제 함수 : 한글 이외의 문자는 전부 제거\n",
    "# 한글만 추출해주는 함수\n",
    "def text_cleaning(text):\n",
    "    # 한글의 정규표현식으로 한글만 추출\n",
    "    hangul = re.compile('[^ ㄱ-ㅣ 가-힣]') # ㄱ-ㅣ 를 지정하지 않으면 ㅋㅋㅋ 같은 것들은 추출되지 않음\n",
    "    result = hangul.sub('', text)\n",
    "    return result\n",
    "\n",
    "# 정규표현식을 이용해서 한글만 추출한 데이터는 ko_text로 저장하고 review 칼럼은 삭제\n",
    "df['ko_text'] = df['review'].apply(lambda x: text_cleaning(x))\n",
    "del df['review']\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 형태소 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['친절하시고/Adjective', '깔끔하고/Adjective', '좋았습니다/Adjective']\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "\n",
    "# konlpy라이브러리로 텍스트 데이터에서 형태소를 추출합니다.\n",
    "def get_pos(x):\n",
    "    tagger = Okt()\n",
    "    pos = tagger.pos(x)\n",
    "    pos = ['{}/{}'.format(word,tag) for word, tag in pos]\n",
    "    return pos\n",
    "\n",
    "# 하나의 데이터로 형태소 추출 동작을 테스트\n",
    "result = get_pos(df['ko_text'][0])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 피처 벡터화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(545, 3030)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# 형태소를 벡터 형태의 학습 데이터셋(X 데이터)으로 변환\n",
    "index_vectorizer = CountVectorizer(tokenizer = lambda x: get_pos(x))\n",
    "X = index_vectorizer.fit_transform(df['ko_text'].tolist())\n",
    "\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 피처 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'친절하시고/Adjective': 2647, '깔끔하고/Adjective': 428, '좋았습니다/Adjective': 2403, '조용하고/Adjective': 2356, '고\n",
      "친절하시고 깔끔하고 좋았습니다\n",
      "  (0, 2647)\t1\n",
      "  (0, 428)\t1\n",
      "  (0, 2403)\t1\n"
     ]
    }
   ],
   "source": [
    "# 피처 확인\n",
    "print(str(index_vectorizer.vocabulary_)[:100])\n",
    "print(df['ko_text'][0])\n",
    "print(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(545, 3030)\n",
      "  (0, 428)\t0.7573091319965198\n",
      "  (0, 2403)\t0.4011484274975501\n",
      "  (0, 2647)\t0.5153278739898709\n"
     ]
    }
   ],
   "source": [
    "# tfidf 벡터화 진행\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "# 형태소를 벡터 형태의 학습 데이터셋(X 데이터)으로 변환\n",
    "tfidf_vectorizer = TfidfTransformer()\n",
    "X = tfidf_vectorizer.fit_transform(X)\n",
    "\n",
    "print(X.shape)\n",
    "print(X[0]) # 가중치 확인 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 학습용 데이터와 훈련용 데이터를 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(381, 3030)\n",
      "(164, 3030)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y = df['y']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 모델 훈련 및 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.90\n",
      "Precision : 0.896\n",
      "Recall : 1.000\n",
      "F1 : 0.945\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# 로지스틱 회귀모델을 학습합니다.\n",
    "lr = LogisticRegression(random_state=42)\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred = lr.predict(X_test)\n",
    "y_pred_probability = lr.predict_proba(X_test)[:,1]\n",
    "\n",
    "# 로지스틱 회귀모델의 성능을 평가합니다.\n",
    "print(\"accuracy: %.2f\" % accuracy_score(y_test, y_pred))\n",
    "print(\"Precision : %.3f\" % precision_score(y_test, y_pred))\n",
    "print(\"Recall : %.3f\" % recall_score(y_test, y_pred))\n",
    "print(\"F1 : %.3f\" % f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0  17]\n",
      " [  0 147]]\n"
     ]
    }
   ],
   "source": [
    "# 오차행렬\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Confusion Matrix를 출력합니다.\n",
    "confmat = confusion_matrix(y_true=y_test, y_pred=y_pred)\n",
    "# 전부 1로 예측\n",
    "print(confmat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 타겟 분포 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    492\n",
       "0     53\n",
       "Name: y, dtype: int64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['y'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UnderSampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90, 3030)\n",
      "(10, 3030)\n"
     ]
    }
   ],
   "source": [
    "# 0인 데이터가 53개가 있으니 1,0을 각각 50개 정도로 샘플링 하기(같은 비율로 추출)\n",
    "# 비율이 높은 데이터(y가 1인 데이터)의 비율을 낮추기(UnderSampling)\n",
    "positive_random_idx = df[df['y']==1].sample(50, random_state=30).index.tolist()\n",
    "negative_random_idx = df[df['y']==0].sample(50, random_state=30).index.tolist()\n",
    "\n",
    "# 랜덤 데이터로 데이터셋을 나누기\n",
    "random_idx = positive_random_idx + negative_random_idx\n",
    "sample_X = X[random_idx, :]\n",
    "y = df['y'][random_idx]\n",
    "X_train, X_test, y_train, y_test = train_test_split(sample_X, y, test_size=0.1)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.80\n",
      "Precision : 0.800\n",
      "Recall : 0.800\n",
      "F1 : 0.800\n"
     ]
    }
   ],
   "source": [
    "# 평가\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# 로지스틱 회귀모델을 학습합니다.\n",
    "lr = LogisticRegression(random_state=42)\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred = lr.predict(X_test)\n",
    "y_pred_probability = lr.predict_proba(X_test)[:,1]\n",
    "\n",
    "# 로지스틱 회귀모델의 성능을 평가합니다.\n",
    "print(\"accuracy: %.2f\" % accuracy_score(y_test, y_pred))\n",
    "print(\"Precision : %.3f\" % precision_score(y_test, y_pred))\n",
    "print(\"Recall : %.3f\" % recall_score(y_test, y_pred))\n",
    "print(\"F1 : %.3f\" % f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4 1]\n",
      " [1 4]]\n"
     ]
    }
   ],
   "source": [
    "# 오차행렬\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Confusion Matrix를 출력합니다.\n",
    "confmat = confusion_matrix(y_true=y_test, y_pred=y_pred)\n",
    "\n",
    "print(confmat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 토픽 모델링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.decomposition import LatentDirichletAllocation # 차원 축소와 관련(기억해두기!)\n",
    "\n",
    "# 데이터를 가져올 카테고리 설정\n",
    "cats = ['rec.motorcycles', 'rec.sport.baseball', 'comp.graphics', 'comp.windows.x',\n",
    "        'talk.politics.mideast', 'soc.religion.christian', 'sci.electronics', 'sci.med']\n",
    "# 카테고리에 해당하는 데이터만 가져오기\n",
    "news_df= fetch_20newsgroups(subset='all',remove=('headers', 'footers', 'quotes'), \n",
    "                            categories=cats, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer Shape: (7862, 1000)\n"
     ]
    }
   ],
   "source": [
    "count_vect = CountVectorizer(max_df=0.95, max_features=1000, min_df=2, stop_words='english', ngram_range=(1,2))\n",
    "feat_vect = count_vect.fit_transform(news_df.data)\n",
    "print('CountVectorizer Shape:', feat_vect.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 1000)\n"
     ]
    }
   ],
   "source": [
    "# 토픽 모델링\n",
    "# 토픽의 개수는 8개\n",
    "lda = LatentDirichletAllocation(n_components=8, random_state=0)\n",
    "lda.fit(feat_vect)\n",
    "print(lda.components_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic # 0\n",
      "year 10 game medical health team 12 20 disease 1993 cancer games years patients 92\n",
      "Topic # 1\n",
      "don just know like said people time think didn ve right going say ll did\n",
      "Topic # 2\n",
      "image file jpeg program gif output format images color files entry 00 use bit 03\n",
      "Topic # 3\n",
      "like think don just use does know good time people used question read point make\n",
      "Topic # 4\n",
      "armenian israel jews armenians turkish people israeli jewish government war dos dos turkey arab 000 armenia\n",
      "Topic # 5\n",
      "edu com available graphics ftp window motif data pub mail widget information use server mit\n",
      "Topic # 6\n",
      "god jesus people church christ believe christian does christians say bible faith sin think paul\n",
      "Topic # 7\n",
      "thanks dos use windows using does help like need display know software server pc problem\n"
     ]
    }
   ],
   "source": [
    "# 각 토픽에서 중요한 10개 단어 추출\n",
    "\n",
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_index, topic in enumerate(model.components_):\n",
    "        print('Topic #',topic_index)\n",
    "\n",
    "        # components_ array에서 가장 값이 큰 순으로 정렬했을 때, 그 값의 array index를 반환. \n",
    "        topic_word_indexes = topic.argsort()[::-1]\n",
    "        top_indexes=topic_word_indexes[:no_top_words]\n",
    "        \n",
    "        # top_indexes대상인 index별로 feature_names에 해당하는 word feature 추출 후 join으로 concat\n",
    "        # 단어 찾아주는 작업\n",
    "        feature_concat = ' '.join([feature_names[i] for i in top_indexes])                \n",
    "        print(feature_concat)\n",
    "\n",
    "# sklearn의 최신 버전에서 get_feature_names 함수가 없어지고 count_vect.get_feature_names_out로 변경\n",
    "# CountVectorizer객체내의 전체 word들의 명칭을 get_features_names( )를 통해 추출\n",
    "feature_names = count_vect.get_feature_names_out()\n",
    "\n",
    "# Topic별 가장 연관도가 높은 word를 15개만 추출\n",
    "display_topics(lda, feature_names, 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 텍스트 군집"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ./data/data/data/OpinosisDataset1.0/topics 디렉토리 안의 파일의 내용을 전부 읽기\n",
    "\n",
    "import glob, os\n",
    "import platform\n",
    "\n",
    "# path = \"\" 대신에 path = None 으로 써도 가능(문자열이기 때문에 가능)\n",
    "path = \"\"\n",
    "if platform.system() == 'Windows':\n",
    "    path = \"data\\data\\data\\OpinosisDataset1.0\\topics\"\n",
    "else : \n",
    "    path = \"data/data/data/OpinosisDataset1.0/topics\"\n",
    "\n",
    "# 디렉토리 이름을 생성\n",
    "#path = \"./data/data/data/OpinosisDataset1.0/topics\"\n",
    "#path = \"data\\data\\data\\OpinosisDataset1.0\\topics\"\n",
    "# 디렉토리 안의 모든 파일이름을 list로 생성\n",
    "all_files = glob.glob(os.path.join(path, \"*.data\"))\n",
    "# print(all_files)\n",
    "\n",
    "# 파일 이름을 저장할 list\n",
    "filename_list = []\n",
    "# 파일 내용을 저장할 list\n",
    "opinion_text = []\n",
    "\n",
    "# 파일 경로를 순회하면서 파일의 내용을 읽어서 하나로 만들기\n",
    "# file은 예약어일 가능성이 높아서 file_로 적음\n",
    "for file_ in all_files:\n",
    "    df = pd.read_table(file_, index_col=None, header=0, encoding='latin1')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문서 군집"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 디렉토리 내의 .data로 끝나는 파일을 모두 읽기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data\\\\data\\\\data\\\\OpinosisDataset1.0\\\\topics\\\\accuracy_garmin_nuvi_255W_gps.txt.data', 'data\\\\data\\\\data\\\\OpinosisDataset1.0\\\\topics\\\\bathroom_bestwestern_hotel_sfo.txt.data', 'data\\\\data\\\\data\\\\OpinosisDataset1.0\\\\topics\\\\battery-life_amazon_kindle.txt.data', 'data\\\\data\\\\data\\\\OpinosisDataset1.0\\\\topics\\\\battery-life_ipod_nano_8gb.txt.data', 'data\\\\data\\\\data\\\\OpinosisDataset1.0\\\\topics\\\\battery-life_netbook_1005ha.txt.data', 'data\\\\data\\\\data\\\\OpinosisDataset1.0\\\\topics\\\\buttons_amazon_kindle.txt.data', 'data\\\\data\\\\data\\\\OpinosisDataset1.0\\\\topics\\\\comfort_honda_accord_2008.txt.data', 'data\\\\data\\\\data\\\\OpinosisDataset1.0\\\\topics\\\\comfort_toyota_camry_2007.txt.data', 'data\\\\data\\\\data\\\\OpinosisDataset1.0\\\\topics\\\\directions_garmin_nuvi_255W_gps.txt.data', 'data\\\\data\\\\data\\\\OpinosisDataset1.0\\\\topics\\\\display_garmin_nuvi_255W_gps.txt.data', 'data\\\\data\\\\data\\\\OpinosisDataset1.0\\\\topics\\\\eyesight-issues_amazon_kindle.txt.data', 'data\\\\data\\\\data\\\\OpinosisDataset1.0\\\\topics\\\\features_windows7.txt.data', 'data\\\\data\\\\data\\\\OpinosisDataset1.0\\\\topics\\\\fonts_amazon_kindle.txt.data', 'data\\\\data\\\\data\\\\OpinosisDataset1.0\\\\topics\\\\food_holiday_inn_london.txt.data', 'data\\\\data\\\\data\\\\OpinosisDataset1.0\\\\topics\\\\food_swissotel_chicago.txt.data', 'data\\\\data\\\\data\\\\OpinosisDataset1.0\\\\topics\\\\free_bestwestern_hotel_sfo.txt.data', 'data\\\\data\\\\data\\\\OpinosisDataset1.0\\\\topics\\\\gas_mileage_toyota_camry_2007.txt.data', 'data\\\\data\\\\data\\\\OpinosisDataset1.0\\\\topics\\\\interior_honda_accord_2008.txt.data', 'data\\\\data\\\\data\\\\OpinosisDataset1.0\\\\topics\\\\interior_toyota_camry_2007.txt.data', 'data\\\\data\\\\data\\\\OpinosisDataset1.0\\\\topics\\\\keyboard_netbook_1005ha.txt.data', 'data\\\\data\\\\data\\\\OpinosisDataset1.0\\\\topics\\\\location_bestwestern_hotel_sfo.txt.data', 'data\\\\data\\\\data\\\\OpinosisDataset1.0\\\\topics\\\\location_holiday_inn_london.txt.data', 'data\\\\data\\\\data\\\\OpinosisDataset1.0\\\\topics\\\\mileage_honda_accord_2008.txt.data', 'data\\\\data\\\\data\\\\OpinosisDataset1.0\\\\topics\\\\navigation_amazon_kindle.txt.data', 'data\\\\data\\\\data\\\\OpinosisDataset1.0\\\\topics\\\\parking_bestwestern_hotel_sfo.txt.data', 'data\\\\data\\\\data\\\\OpinosisDataset1.0\\\\topics\\\\performance_honda_accord_2008.txt.data', 'data\\\\data\\\\data\\\\OpinosisDataset1.0\\\\topics\\\\performance_netbook_1005ha.txt.data', 'data\\\\data\\\\data\\\\OpinosisDataset1.0\\\\topics\\\\price_amazon_kindle.txt.data', 'data\\\\data\\\\data\\\\OpinosisDataset1.0\\\\topics\\\\price_holiday_inn_london.txt.data', 'data\\\\data\\\\data\\\\OpinosisDataset1.0\\\\topics\\\\quality_toyota_camry_2007.txt.data', 'data\\\\data\\\\data\\\\OpinosisDataset1.0\\\\topics\\\\rooms_bestwestern_hotel_sfo.txt.data', 'data\\\\data\\\\data\\\\OpinosisDataset1.0\\\\topics\\\\rooms_swissotel_chicago.txt.data', 'data\\\\data\\\\data\\\\OpinosisDataset1.0\\\\topics\\\\room_holiday_inn_london.txt.data', 'data\\\\data\\\\data\\\\OpinosisDataset1.0\\\\topics\\\\satellite_garmin_nuvi_255W_gps.txt.data', 'data\\\\data\\\\data\\\\OpinosisDataset1.0\\\\topics\\\\screen_garmin_nuvi_255W_gps.txt.data', 'data\\\\data\\\\data\\\\OpinosisDataset1.0\\\\topics\\\\screen_ipod_nano_8gb.txt.data', 'data\\\\data\\\\data\\\\OpinosisDataset1.0\\\\topics\\\\screen_netbook_1005ha.txt.data', 'data\\\\data\\\\data\\\\OpinosisDataset1.0\\\\topics\\\\seats_honda_accord_2008.txt.data', 'data\\\\data\\\\data\\\\OpinosisDataset1.0\\\\topics\\\\service_bestwestern_hotel_sfo.txt.data', 'data\\\\data\\\\data\\\\OpinosisDataset1.0\\\\topics\\\\service_holiday_inn_london.txt.data', 'data\\\\data\\\\data\\\\OpinosisDataset1.0\\\\topics\\\\service_swissotel_hotel_chicago.txt.data', 'data\\\\data\\\\data\\\\OpinosisDataset1.0\\\\topics\\\\size_asus_netbook_1005ha.txt.data', 'data\\\\data\\\\data\\\\OpinosisDataset1.0\\\\topics\\\\sound_ipod_nano_8gb.txt.data', 'data\\\\data\\\\data\\\\OpinosisDataset1.0\\\\topics\\\\speed_garmin_nuvi_255W_gps.txt.data', 'data\\\\data\\\\data\\\\OpinosisDataset1.0\\\\topics\\\\speed_windows7.txt.data', 'data\\\\data\\\\data\\\\OpinosisDataset1.0\\\\topics\\\\staff_bestwestern_hotel_sfo.txt.data', 'data\\\\data\\\\data\\\\OpinosisDataset1.0\\\\topics\\\\staff_swissotel_chicago.txt.data', 'data\\\\data\\\\data\\\\OpinosisDataset1.0\\\\topics\\\\transmission_toyota_camry_2007.txt.data', 'data\\\\data\\\\data\\\\OpinosisDataset1.0\\\\topics\\\\updates_garmin_nuvi_255W_gps.txt.data', 'data\\\\data\\\\data\\\\OpinosisDataset1.0\\\\topics\\\\video_ipod_nano_8gb.txt.data', 'data\\\\data\\\\data\\\\OpinosisDataset1.0\\\\topics\\\\voice_garmin_nuvi_255W_gps.txt.data']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob, os\n",
    "\n",
    "# 데이터 파일이 있는 디렉토리 경로를 생성\n",
    "path = \"data\\\\data\\\\data\\\\OpinosisDataset1.0\\\\topics\"\n",
    "\n",
    "# 디렉토리 내의 .data로 끝나는 모든 파일의 이름을 가져오기\n",
    "all_files = glob.glob(os.path.join(path,\"*.data\"))\n",
    "print(all_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['accuracy_garmin_nuvi_255W_gps', 'bathroom_bestwestern_hotel_sfo', 'battery-life_amazon_kindle', 'battery-life_ipod_nano_8gb', 'battery-life_netbook_1005ha', 'buttons_amazon_kindle', 'comfort_honda_accord_2008', 'comfort_toyota_camry_2007', 'directions_garmin_nuvi_255W_gps', 'display_garmin_nuvi_255W_gps', 'eyesight-issues_amazon_kindle', 'features_windows7', 'fonts_amazon_kindle', 'food_holiday_inn_london', 'food_swissotel_chicago', 'free_bestwestern_hotel_sfo', 'gas_mileage_toyota_camry_2007', 'interior_honda_accord_2008', 'interior_toyota_camry_2007', 'keyboard_netbook_1005ha', 'location_bestwestern_hotel_sfo', 'location_holiday_inn_london', 'mileage_honda_accord_2008', 'navigation_amazon_kindle', 'parking_bestwestern_hotel_sfo', 'performance_honda_accord_2008', 'performance_netbook_1005ha', 'price_amazon_kindle', 'price_holiday_inn_london', 'quality_toyota_camry_2007', 'rooms_bestwestern_hotel_sfo', 'rooms_swissotel_chicago', 'room_holiday_inn_london', 'satellite_garmin_nuvi_255W_gps', 'screen_garmin_nuvi_255W_gps', 'screen_ipod_nano_8gb', 'screen_netbook_1005ha', 'seats_honda_accord_2008', 'service_bestwestern_hotel_sfo', 'service_holiday_inn_london', 'service_swissotel_hotel_chicago', 'size_asus_netbook_1005ha', 'sound_ipod_nano_8gb', 'speed_garmin_nuvi_255W_gps', 'speed_windows7', 'staff_bestwestern_hotel_sfo', 'staff_swissotel_chicago', 'transmission_toyota_camry_2007', 'updates_garmin_nuvi_255W_gps', 'video_ipod_nano_8gb', 'voice_garmin_nuvi_255W_gps']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>opinion_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>accuracy_garmin_nuvi_255W_gps</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bathroom_bestwestern_hotel_sfo</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>battery-life_amazon_kindle</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>battery-life_ipod_nano_8gb</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>battery-life_netbook_1005ha</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         filename  \\\n",
       "0   accuracy_garmin_nuvi_255W_gps   \n",
       "1  bathroom_bestwestern_hotel_sfo   \n",
       "2      battery-life_amazon_kindle   \n",
       "3      battery-life_ipod_nano_8gb   \n",
       "4     battery-life_netbook_1005ha   \n",
       "\n",
       "                                        opinion_text  \n",
       "0                                                ...  \n",
       "1                                                ...  \n",
       "2                                                ...  \n",
       "3                                                ...  \n",
       "4                                                ...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 파일의 이름과 내용을 저장할 list\n",
    "filename_list = []\n",
    "opinion_text = []\n",
    "\n",
    "for file_ in all_files:\n",
    "    df = pd.read_table(file_, index_col = None, header=0, encoding=\"latin1\")\n",
    "\n",
    "    # 파일 이름만 추출\n",
    "    filename_ = file_.split('\\\\')[-1]\n",
    "    filename = filename_.split(\".\")[0]\n",
    "\n",
    "    # 파일명과 내용을 list에 저장\n",
    "    filename_list.append(filename)\n",
    "    opinion_text.append(df.to_string())\n",
    "\n",
    "print(filename_list)\n",
    "\n",
    "# 파일 이름과 내용으로 DataFrame을 생성\n",
    "document_df = pd.DataFrame({'filename':filename_list,'opinion_text':opinion_text})\n",
    "document_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 피처 벡터화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "c:\\Users\\USER\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1465)\t0.023283541665665836\n",
      "  (0, 385)\t0.023283541665665836\n",
      "  (0, 1470)\t0.05108766228354363\n",
      "  (0, 2652)\t0.023283541665665836\n",
      "  (0, 1357)\t0.02182613746339447\n",
      "  (0, 723)\t0.023283541665665836\n",
      "  (0, 4367)\t0.02182613746339447\n",
      "  (0, 4593)\t0.01798716138533979\n",
      "  (0, 4273)\t0.02063535152550281\n",
      "  (0, 1468)\t0.019628556360506585\n",
      "  (0, 1804)\t0.019628556360506585\n",
      "  (0, 4040)\t0.023283541665665836\n",
      "  (0, 1467)\t0.015101445414252302\n",
      "  (0, 2469)\t0.023283541665665836\n",
      "  (0, 2173)\t0.02182613746339447\n",
      "  (0, 4191)\t0.02182613746339447\n",
      "  (0, 4196)\t0.019628556360506585\n",
      "  (0, 234)\t0.023283541665665836\n",
      "  (0, 4041)\t0.023283541665665836\n",
      "  (0, 4092)\t0.04127070305100562\n",
      "  (0, 1533)\t0.04365227492678894\n",
      "  (0, 1996)\t0.023283541665665836\n",
      "  (0, 158)\t0.02182613746339447\n",
      "  (0, 1164)\t0.023283541665665836\n",
      "  (0, 3130)\t0.02182613746339447\n",
      "  :\t:\n",
      "  (50, 2028)\t0.01659453509406821\n",
      "  (50, 2037)\t0.009111906180203912\n",
      "  (50, 1655)\t0.009761754136944606\n",
      "  (50, 1321)\t0.009452031092583052\n",
      "  (50, 3294)\t0.022126046792090945\n",
      "  (50, 3430)\t0.017333161159535555\n",
      "  (50, 3911)\t0.012364292655146337\n",
      "  (50, 1823)\t0.004981407956984028\n",
      "  (50, 17)\t0.005191386785643596\n",
      "  (50, 3100)\t0.006042182877162159\n",
      "  (50, 896)\t0.004880877068472303\n",
      "  (50, 4228)\t0.03887821689729189\n",
      "  (50, 1894)\t0.04914010869718211\n",
      "  (50, 4265)\t0.004504886421618933\n",
      "  (50, 2651)\t0.026404189389097856\n",
      "  (50, 3537)\t0.03406302085057687\n",
      "  (50, 555)\t0.011925603428065111\n",
      "  (50, 3952)\t0.007161504548169905\n",
      "  (50, 3565)\t0.005084853186647257\n",
      "  (50, 2737)\t0.007774771186628816\n",
      "  (50, 2421)\t0.007774771186628816\n",
      "  (50, 1396)\t0.004330773327433595\n",
      "  (50, 1353)\t0.0982595317876811\n",
      "  (50, 1803)\t0.06879615217605495\n",
      "  (50, 484)\t0.017602792926065237\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "import string\n",
    "\n",
    "# 구두점 제거\n",
    "remove_punch_dict = dict((ord(punch),None) for punch in string.punctuation)\n",
    "#print(remove_punch_dict)\n",
    "lemmar = WordNetLemmatizer()\n",
    "\n",
    "# 문장을 토큰화 한 후 어근을 찾아오는 함수\n",
    "def LemTokens(tokens):\n",
    "    return [lemmar.lemmatize(token) for token in tokens]\n",
    "\n",
    "def LemNormalize(text):\n",
    "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punch_dict)))\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vect = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english',\n",
    "                               ngram_range=(1,2), min_df=0.05, max_df=0.85)\n",
    "feature_vect = tfidf_vect.fit_transform(document_df['opinion_text'])\n",
    "\n",
    "# 문자에 어떤 단어가 얼마의 가중치를 가지고 있는지 희소 행렬로 생성\n",
    "print(feature_vect)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 군집 알고리즘 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>opinion_text</th>\n",
       "      <th>cluster_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>accuracy_garmin_nuvi_255W_gps</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bathroom_bestwestern_hotel_sfo</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>battery-life_amazon_kindle</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>battery-life_ipod_nano_8gb</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>battery-life_netbook_1005ha</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         filename  \\\n",
       "0   accuracy_garmin_nuvi_255W_gps   \n",
       "1  bathroom_bestwestern_hotel_sfo   \n",
       "2      battery-life_amazon_kindle   \n",
       "3      battery-life_ipod_nano_8gb   \n",
       "4     battery-life_netbook_1005ha   \n",
       "\n",
       "                                        opinion_text  cluster_label  \n",
       "0                                                ...              2  \n",
       "1                                                ...              0  \n",
       "2                                                ...              1  \n",
       "3                                                ...              1  \n",
       "4                                                ...              1  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# 5개의 군집을 위한 인스턴스 생성\n",
    "km_cluster = KMeans(n_clusters=5, max_iter=10000, random_state=0)\n",
    "# 훈련\n",
    "km_cluster.fit(feature_vect)\n",
    "# 각 데이터의 레이블을 저장\n",
    "cluster_label=km_cluster.labels_\n",
    "# 각 군집의 중앙점을 저장\n",
    "cluster_centers = km_cluster.cluster_centers_\n",
    "\n",
    "document_df['cluster_label'] = cluster_label\n",
    "document_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>opinion_text</th>\n",
       "      <th>cluster_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bathroom_bestwestern_hotel_sfo</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>room_holiday_inn_london</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>rooms_bestwestern_hotel_sfo</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>rooms_swissotel_chicago</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          filename  \\\n",
       "1   bathroom_bestwestern_hotel_sfo   \n",
       "32         room_holiday_inn_london   \n",
       "30     rooms_bestwestern_hotel_sfo   \n",
       "31         rooms_swissotel_chicago   \n",
       "\n",
       "                                         opinion_text  cluster_label  \n",
       "1                                                 ...              0  \n",
       "32                                                ...              0  \n",
       "30                                                ...              0  \n",
       "31                                                ...              0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_df[document_df['cluster_label']==0].sort_values(by='filename')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>opinion_text</th>\n",
       "      <th>cluster_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>accuracy_garmin_nuvi_255W_gps</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bathroom_bestwestern_hotel_sfo</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>battery-life_amazon_kindle</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>battery-life_ipod_nano_8gb</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>battery-life_netbook_1005ha</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         filename  \\\n",
       "0   accuracy_garmin_nuvi_255W_gps   \n",
       "1  bathroom_bestwestern_hotel_sfo   \n",
       "2      battery-life_amazon_kindle   \n",
       "3      battery-life_ipod_nano_8gb   \n",
       "4     battery-life_netbook_1005ha   \n",
       "\n",
       "                                        opinion_text  cluster_label  \n",
       "0                                                ...              0  \n",
       "1                                                ...              2  \n",
       "2                                                ...              0  \n",
       "3                                                ...              0  \n",
       "4                                                ...              0  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 군집 개수 변경(5개->3개)\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# 3개의 군집을 위한 인스턴스 생성\n",
    "km_cluster = KMeans(n_clusters=3, max_iter=10000, random_state=0)\n",
    "# 훈련\n",
    "km_cluster.fit(feature_vect)\n",
    "# 각 데이터의 레이블을 저장\n",
    "cluster_label=km_cluster.labels_\n",
    "# 각 군집의 중앙점을 저장\n",
    "cluster_centers = km_cluster.cluster_centers_\n",
    "\n",
    "document_df['cluster_label'] = cluster_label\n",
    "document_df.sort_values(by='cluster_label')\n",
    "document_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 군집을 이루게 만든 핵심 단어 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 클러스터링 모델과 데이터 그리고 피처 이름, 클러스터 개수와 추출할 핵심 단어 개수를 받아서 리턴하는 함수\n",
    "def get_cluster_details(cluster_model, cluster_data, feature_names, clusters_num,\n",
    "                        top_n_features=10):\n",
    "    cluster_details={}\n",
    "\n",
    "    # 군집 중심과의 거리가 먼 단어 순으로 저장하기\n",
    "    centroid_feature_ordered_ind = cluster_model.cluster_centers_.argsort()[:,::-1]\n",
    "\n",
    "    for cluster_num in range(clusters_num):\n",
    "        # 딕셔터리에다가 하나씩 집어넣기\n",
    "        cluster_details[cluster_num] = {}\n",
    "        cluster_details[cluster_num]['cluster'] = cluster_num\n",
    "        # 중요 피처를 추출\n",
    "        top_feature_indexes = centroid_feature_ordered_ind[cluster_num, :top_n_features]\n",
    "        top_features = [feature_names[ind] for ind in top_feature_indexes]\n",
    "        # 중앙점과의 거리 저장\n",
    "        top_feature_values = cluster_model.cluster_centers_[cluster_num, top_feature_indexes].tolist()\n",
    "\n",
    "        cluster_details[cluster_num]['top_features'] = top_features\n",
    "        cluster_details[cluster_num]['top_features_value'] = top_feature_values\n",
    "        filenames = cluster_data[cluster_data['cluster_label'] == cluster_num]['filename']\n",
    "        filenames = filenames.values.tolist()\n",
    "        cluster_details[cluster_num]['filenames'] = filenames\n",
    "\n",
    "    return cluster_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 클러스터별 핵심 단어를 출력하는 함수\n",
    "\n",
    "def print_cluster_details(cluster_details):\n",
    "    for cluster_num, cluster_detail in cluster_details.items():\n",
    "        print('#######', cluster_num, '#######')\n",
    "        print('핵심 단어:', cluster_detail['top_features'])\n",
    "        print('파일명:',cluster_detail['filenames'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####### 0 #######\n",
      "핵심 단어: ['screen', 'battery', 'keyboard', 'battery life', 'life', 'kindle', 'direction', 'video', 'size', 'voice']\n",
      "파일명: ['accuracy_garmin_nuvi_255W_gps', 'battery-life_amazon_kindle', 'battery-life_ipod_nano_8gb', 'battery-life_netbook_1005ha', 'buttons_amazon_kindle', 'directions_garmin_nuvi_255W_gps', 'display_garmin_nuvi_255W_gps', 'eyesight-issues_amazon_kindle', 'features_windows7', 'fonts_amazon_kindle', 'keyboard_netbook_1005ha', 'navigation_amazon_kindle', 'performance_netbook_1005ha', 'price_amazon_kindle', 'satellite_garmin_nuvi_255W_gps', 'screen_garmin_nuvi_255W_gps', 'screen_ipod_nano_8gb', 'screen_netbook_1005ha', 'size_asus_netbook_1005ha', 'sound_ipod_nano_8gb', 'speed_garmin_nuvi_255W_gps', 'speed_windows7', 'updates_garmin_nuvi_255W_gps', 'video_ipod_nano_8gb', 'voice_garmin_nuvi_255W_gps']\n",
      "####### 1 #######\n",
      "핵심 단어: ['interior', 'seat', 'mileage', 'comfortable', 'gas', 'gas mileage', 'transmission', 'car', 'performance', 'quality']\n",
      "파일명: ['comfort_honda_accord_2008', 'comfort_toyota_camry_2007', 'gas_mileage_toyota_camry_2007', 'interior_honda_accord_2008', 'interior_toyota_camry_2007', 'mileage_honda_accord_2008', 'performance_honda_accord_2008', 'quality_toyota_camry_2007', 'seats_honda_accord_2008', 'transmission_toyota_camry_2007']\n",
      "####### 2 #######\n",
      "핵심 단어: ['room', 'hotel', 'service', 'staff', 'food', 'location', 'bathroom', 'clean', 'price', 'parking']\n",
      "파일명: ['bathroom_bestwestern_hotel_sfo', 'food_holiday_inn_london', 'food_swissotel_chicago', 'free_bestwestern_hotel_sfo', 'location_bestwestern_hotel_sfo', 'location_holiday_inn_london', 'parking_bestwestern_hotel_sfo', 'price_holiday_inn_london', 'rooms_bestwestern_hotel_sfo', 'rooms_swissotel_chicago', 'room_holiday_inn_london', 'service_bestwestern_hotel_sfo', 'service_holiday_inn_london', 'service_swissotel_hotel_chicago', 'staff_bestwestern_hotel_sfo', 'staff_swissotel_chicago']\n"
     ]
    }
   ],
   "source": [
    "# 피처 이름을 전부 가져오기\n",
    "feature_names = tfidf_vect.get_feature_names_out()\n",
    "#print(feature_names)\n",
    "cluster_details = get_cluster_details(cluster_model=km_cluster, cluster_data=document_df,\n",
    "                                  feature_names=feature_names, clusters_num=3, top_n_features=10)\n",
    "print_cluster_details(cluster_details)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 코사인 유사도를 구하는 알고리즘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 코사인 유사도를 계산해주는 함수\n",
    "def cos_similarity(v1, v2) :\n",
    "    dot_product = np.dot(v1,v2)\n",
    "    l2_norm = (np.sqrt(sum(np.square(v1))) * np.sqrt(sum(np.square(v2))))\n",
    "    similarity = dot_product / l2_norm\n",
    "\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 2)\t0.41556360057939173\n",
      "  (0, 13)\t0.41556360057939173\n",
      "  (0, 8)\t0.24543855687841593\n",
      "  (0, 0)\t0.41556360057939173\n",
      "  (0, 15)\t0.49087711375683185\n",
      "  (0, 14)\t0.24543855687841593\n",
      "  (0, 17)\t0.24543855687841593\n",
      "  (0, 6)\t0.24543855687841593\n",
      "==\n",
      "  (0, 16)\t0.39624495215024286\n",
      "  (0, 7)\t0.39624495215024286\n",
      "  (0, 12)\t0.39624495215024286\n",
      "  (0, 10)\t0.3013544995034864\n",
      "  (0, 8)\t0.2340286519091622\n",
      "  (0, 15)\t0.2340286519091622\n",
      "  (0, 14)\t0.2340286519091622\n",
      "  (0, 17)\t0.4680573038183244\n",
      "  (0, 6)\t0.2340286519091622\n",
      "==\n",
      "[[0.4155636  0.         0.4155636  0.         0.         0.\n",
      "  0.24543856 0.         0.24543856 0.         0.         0.\n",
      "  0.         0.4155636  0.24543856 0.49087711 0.         0.24543856]]\n",
      "==\n",
      "[[0.         0.         0.         0.         0.         0.\n",
      "  0.23402865 0.39624495 0.23402865 0.         0.3013545  0.\n",
      "  0.39624495 0.         0.23402865 0.23402865 0.39624495 0.4680573 ]]\n"
     ]
    }
   ],
   "source": [
    "doc_list = ['if you take the blue pill, the story ends',\n",
    "            'if you take the red pill, you stay in Wonderland',\n",
    "            'if you take the red pill, i show you how deep rabbit hole goes']\n",
    "\n",
    "tfidf_vect_simple = TfidfVectorizer()\n",
    "feature_vect_simple = tfidf_vect_simple.fit_transform(doc_list)\n",
    "\n",
    "# 현재 결과는 희소 행렬이라서 거리 계산을 못함\n",
    "# 단어의 개수가 달라서 계산 불가(각각의 피처 개수가 다름)\n",
    "print(feature_vect_simple[0])\n",
    "print(\"==\")\n",
    "print(feature_vect_simple[1])\n",
    "\n",
    "# 밀집 배열로 변환(피처의 개수가 같아졌음 : 거리 계산 가능)\n",
    "feature_vect_dense = feature_vect_simple.todense()\n",
    "print(\"==\")\n",
    "print(feature_vect_dense[0])\n",
    "print(\"==\")\n",
    "print(feature_vect_dense[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장 1과 문장2의 유사도: 0.4020775821495014\n",
      "문장 1과 문장3의 유사도: 0.33151185783991466\n",
      "문장 2과 문장3의 유사도: 0.4361341528640473\n"
     ]
    }
   ],
   "source": [
    "# 거리 계산\n",
    "import numpy as np\n",
    "\n",
    "vect1 = np.array(feature_vect_dense[0]).reshape(-1,)\n",
    "vect2 = np.array(feature_vect_dense[1]).reshape(-1,)\n",
    "vect3 = np.array(feature_vect_dense[2]).reshape(-1,)\n",
    "\n",
    "print(\"문장 1과 문장2의 유사도:\", cos_similarity(vect1, vect2))\n",
    "print(\"문장 1과 문장3의 유사도:\", cos_similarity(vect1, vect3))\n",
    "print(\"문장 2과 문장3의 유사도:\", cos_similarity(vect2, vect3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API를 활용한 코사인 유사도 구하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.40207758 0.33151186]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "similarity_simple_pair = cosine_similarity(feature_vect_simple[0],feature_vect_simple)\n",
    "\n",
    "print(similarity_simple_pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문서 군집의 코사인 유사도 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.01005322 0.         0.         ... 0.00706287 0.         0.        ]\n",
      " [0.         0.00092551 0.         ... 0.         0.         0.        ]\n",
      " [0.         0.00099499 0.00174637 ... 0.         0.00183397 0.00144581]]\n"
     ]
    }
   ],
   "source": [
    "km_cluster = KMeans(n_clusters=3, max_iter=10000, random_state=0)\n",
    "km_cluster.fit(feature_vect)\n",
    "cluster_label = km_cluster.labels_\n",
    "# 각 군집의 중앙점(centroid)\n",
    "cluster_centers = km_cluster.cluster_centers_\n",
    "document_df['cluster_label'] = cluster_label\n",
    "print(cluster_centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1번 클러스터의 문서 인덱스: Int64Index([6, 7, 16, 17, 18, 22, 25, 29, 37, 47], dtype='int64')\n",
      "[[1.         0.83969704 0.15655631 0.33044002 0.25981841 0.16544257\n",
      "  0.27569738 0.18050974 0.65502034 0.06229873]]\n",
      "[[0.02315315 0.03787848 0.01383035 0.01725198 0.01973879 0.01873283\n",
      "  1.         0.83969704]]\n"
     ]
    }
   ],
   "source": [
    "# 1번 클러스터의 문서들 간의 코사인 유사도 확인\n",
    "hotel_indexes = document_df[document_df['cluster_label']==1].index\n",
    "print(\"1번 클러스터의 문서 인덱스:\", hotel_indexes)\n",
    "\n",
    "\n",
    "# 자신의 군집에 있는 문서와의 코사인 유사도\n",
    "similarity_pair = cosine_similarity(feature_vect[hotel_indexes[0]],\n",
    "                                    feature_vect[hotel_indexes])\n",
    "print(similarity_pair)\n",
    "\n",
    "# 0번 부터 7번 까지 문서와 코사인 유사도(6,7번째만 1번 클러스트이고 나머지는 아님)\n",
    "similarity_pair = cosine_similarity(feature_vect[hotel_indexes[0]],\n",
    "                                    feature_vect[[0,1,2,3,4,5,6,7]])\n",
    "print(similarity_pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 한글 문서의 유사도 측정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['우리', '과일', '먹으러', '가자'], ['오늘', '은', '목요일', '입니다', '.'], ['나', '는', '공원', '에서', '산책', '하는', '것', '을', '싫어합니다', '.'], ['나', '는', '거리', '를', '걷는', '것', '을', '좋아합니다', '.'], ['걷는', '것', '이', '프로그래밍', '보다', '재미있습니다', '.']]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# Twitter 에서 Okt로 바꼈다는 경고가 떠서 Okt로 바꿔줌\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "okt = Okt()\n",
    "vectorizer = CountVectorizer(min_df=1)\n",
    "contents = ['우리 과일 먹으러 가자', '오늘은 목요일입니다.',\n",
    "            '나는 공원에서 산책하는 것을 싫어합니다.',\n",
    "            '나는 거리를 걷는 것을 좋아합니다.',\n",
    "            '걷는 것이 프로그래밍보다 재미있습니다.']\n",
    "\n",
    "# 한글 토큰화\n",
    "contents_tokens = [okt.morphs(row) for row in contents]\n",
    "print(contents_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' 우리 과일 먹으러 가자', ' 오늘 은 목요일 입니다 .', ' 나 는 공원 에서 산책 하는 것 을 싫어합니다 .', ' 나 는 거리 를 걷는 것 을 좋아합니다 .', ' 걷는 것 이 프로그래밍 보다 재미있습니다 .']\n"
     ]
    }
   ],
   "source": [
    "# 토큰화된 결과를 가지고 다시 문장을 생성 - 피처 벡터화 때문\n",
    "contents_for_vectorize=[]\n",
    "for content in contents_tokens:\n",
    "    sentence = ''\n",
    "    for word in content:\n",
    "        sentence = sentence + ' ' + word\n",
    "    contents_for_vectorize.append(sentence)\n",
    "print(contents_for_vectorize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['가자' '거리' '걷는' '공원' '과일' '먹으러' '목요일' '보다' '산책' '싫어합니다' '에서' '오늘' '우리'\n",
      " '입니다' '재미있습니다' '좋아합니다' '프로그래밍' '하는']\n",
      "[[1 0 0 0 0]\n",
      " [0 0 0 1 0]\n",
      " [0 0 0 1 1]\n",
      " [0 0 1 0 0]\n",
      " [1 0 0 0 0]\n",
      " [1 0 0 0 0]\n",
      " [0 1 0 0 0]\n",
      " [0 0 0 0 1]\n",
      " [0 0 1 0 0]\n",
      " [0 0 1 0 0]\n",
      " [0 0 1 0 0]\n",
      " [0 1 0 0 0]\n",
      " [1 0 0 0 0]\n",
      " [0 1 0 0 0]\n",
      " [0 0 0 0 1]\n",
      " [0 0 0 1 0]\n",
      " [0 0 0 0 1]\n",
      " [0 0 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# 피처 벡터화\n",
    "X = vectorizer.fit_transform(contents_for_vectorize)\n",
    "\n",
    "# 피처 확인\n",
    "print(vectorizer.get_feature_names_out())\n",
    "\n",
    "# 문장의 피처 벡터화 된 후의 결과 확인\n",
    "print(X.toarray().transpose())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' 우리 과 이 먹으러 갖']\n"
     ]
    }
   ],
   "source": [
    "# 유사도를 측정할 데이터 생성\n",
    "# 토큰화 후 다시 문장 생성\n",
    "new_post = ['우리 과이 먹으러 갖']\n",
    "new_post_tokens = [okt.morphs(row) for row in new_post]\n",
    "\n",
    "new_post_for_vectorize = []\n",
    "\n",
    "for content in new_post_tokens:\n",
    "    sentence = ''\n",
    "    for word in content:\n",
    "        sentence = sentence + ' ' + word\n",
    "        \n",
    "    new_post_for_vectorize.append(sentence)\n",
    "    \n",
    "print(new_post_for_vectorize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 5)\t1\n",
      "  (0, 12)\t1\n",
      "[[0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# 테스트 데이터의 피처 벡터화\n",
    "new_post_vec = vectorizer.transform(new_post_for_vectorize)\n",
    "print(new_post_vec)\n",
    "print(new_post_vec.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 거리 구하는 함수 만들기\n",
    "import scipy as sp\n",
    "\n",
    "def dist_raw(v1, v2):\n",
    "    delta = v1 - v2\n",
    "    return sp.linalg.norm(delta.toarray())\n",
    "\n",
    "best_doc = None\n",
    "best_dist = 65535\n",
    "best_i = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 번째 문장과의 거리: 1.4142135623730951   우리 과일 먹으러 가자\n",
      "1 번째 문장과의 거리: 2.23606797749979   오늘은 목요일입니다.\n",
      "2 번째 문장과의 거리: 2.6457513110645907   나는 공원에서 산책하는 것을 싫어합니다.\n",
      "3 번째 문장과의 거리: 2.23606797749979   나는 거리를 걷는 것을 좋아합니다.\n",
      "4 번째 문장과의 거리: 2.449489742783178   걷는 것이 프로그래밍보다 재미있습니다.\n"
     ]
    }
   ],
   "source": [
    "# 다른 문장들과의 거리\n",
    "best_doc = None\n",
    "best_dist = 65535\n",
    "best_i = None\n",
    "\n",
    "for i in range(0,5):\n",
    "    post_vec = X.getrow(i)\n",
    "    d = dist_raw(post_vec, new_post_vec)\n",
    "\n",
    "    print(i, \"번째 문장과의 거리:\", d, \" \", contents[i])\n",
    "\n",
    "    if d < best_dist:\n",
    "        best_dist = d\n",
    "        best_i = i\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "우리 과일 먹으러 가자\n"
     ]
    }
   ],
   "source": [
    "print(contents[best_i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
